
---
title: "Final Report"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(table1)
```

# Background

Climate change is very prevalent today and affects every inhabited region in the world. Evidence of climate change is shown by rising sea levels, increased extreme weather, and warmer surface and ocean temperatures. Regarding global surface temperatures, NOAA’s 2021 Annual Climate Report states that they increase annually and are projected to continue increasing. Due to the uneven nature of warming, average temperatures will reflect the average global temperature, not the temperature in a specific geographic location. We want to be able to visualize and communicate data that we have gathered to various stakeholders. 

Our final project aims to increase the ease of investigators to access data and statistical methods. We have created an app that allows people to visualize the change in average temperature historically, as well as the predicted temperature changes for that country out 5 years.

# Data

The data are collected from UC-Berkley’s climate lab, spanning 1750-2013. It is publicly available on Kaggle under "Climate Change: Earth Surface Temperature Data". Specifically, we are utilizing the Global Land Temperatures By City (GlobalLandTemperaturesByCity.csv) file. The variables of interest are Average Temperature, Latitude, Longitude, City, Country, and Date. In the 1980's, Average Temperature started being measured by a digital thermometer which may make recent measurements more accurate, but reflect a cooling bias relative to older measures.

### Data preview:

```{r, include=FALSE}
library(tidyverse)
library(data.table)
setwd("/home/gtiernon/biostat625/project")
env_data <- fread("~/GlobalLandTemperaturesByCity.csv", showProgress = F) %>%
select(-V1) %>%
na.omit()
```

```{r}
head(env_data)
dim(env_data)
```

Below are tables with randomly sampled countries in our dataset. We stratify by city to give examples of the different numbers of measurement points in our data.

```{r, warning=FALSE}
smallNumObs = env_data[which(env_data$Country == "Croatia"),]
t1kable((table1(~year + AverageTemperature  |City, data = smallNumObs)))
smallNumObs = env_data[which(env_data$Country == "Namibia"),]
t1kable((table1(~year + AverageTemperature  |City, data = smallNumObs)))
```


# Research Aim

Our goal is to communicate time series forecasting of global surface temperatures to policy makers, or curious citizens, through interactive visualizations on a user-friendly Shiny application. 

# Analysis

### How we built the App

We started with a skeleton of the side bar and tab panels. We used various selection options in R shiny to add graphical options for the user, including selectInput(), checkboxGroupInput(), sliderInput(), etc... We populated selectinput with countries and cities, where the cities reactively changed based on the country selected. Many conditional panels were used to show/hide options based on the specific graph the user wanted to play with. Dates were controlled with the sliderInput() and certain graphical options like stratification and lowess line were managed by checkboxGroupInput(). There were 3 main tabs all corresponding to different ways of looking at the data: by country, by city or by predictive forecasting. The main panel graphs were made using ggplot and shown based on the tab the user selected. Custom graph options included lowess lines, x axis angle, year/month ranges and stratification of country by city. The download button was made using downloadHandler() and using last_plot() in the content argument. User can choose between PDF and PNG through a reactive statement based on a radiobutton. That reactive then takes on a pdf or png value and is called withing downloadHandler as well to specify type of download. Filename was also set to the date for easy searching for the user. Lastly, bindCache was used to improve the performance of the app over time as many more users interact with the app by caching inputs and reactives once they are accessed once and grabbing them again when needed instead of reloading those inputs all over again.

### ARIMA

To make predictions about future temperature trends, we utilized the ARIMA (AutoRegressive Integrated Moving Average) model which is a statistical method for time series forecasting. In the R package 'forecast', we utilized the auto.arima() function, which returns the best ARIMA model according to either AIC or BIC values. This model has the advantage of automatically choosing a model that include parameters that make the most sense. ARIMA models has three parameters that you can fit to them, and because we have 159 countries, we wanted the auto.arima() function to automate the process of fitting time series models.

When forecasting the time series model, there were a lot of choices to make about how much data to use, how much data to display, and what summary measures we wanted to to use to forecast. For example, the data have measurements every month for most of the years from 1900 to 2013. Over that time span, there are possibly $113 \times 12=1356$ possible measurements to make for each measurement location (i.e. a country can have upwards of 200 possible measurement cities), so displaying all those possible data points in a time series plot, got really dense, really fast, and often times looked like noise with no signal. 

To deal with the question of display, we chose to take the average temperature of each country across measurement sites, for each day-time object that we have in the dataset. So for Afghanistan, which has cities Baglan, Gardez, Gazni, Herat,Jalalabad, Kabul, Qandahar, and Qunduz, we took the average of those measurements for Januaray 1st, 1900, February 1st 1900, and so on and so forth up until the last time point in the dataset. We then fit the ARIMA model to these "national average" temperature measurements. However, that still resulted in a lot of points in the time series plot, so we chose for the sake of visualization to only plot the national averages since 2008. This is another potential big data application, though it is one that we never covered in class. It is more about scientific communication and derived variables.

One of the big questions of climate change is how to forecast the way that temperatures will change over time. We want to be able to predict the way that countries will be affected by climate change. As a result, we had the ARIMA model forecast out five years beyond the end of the data. The visualization includes the error bars that result in this predication, andf give the viewer the possibility to think about the future effects of climate change in a specific nation-state.

A potential area for further study would be to include a covariate with relevant historical predictive power, such as global greenhouse gas output. This would likely give us a better forecast and model so that we could get less variance on our estimates, and increase precision of our forecasts. Additionally, if we knew exactly when a site switched over to digital thermometer use, we would be able to include that as an indicator variable and possibly estimate the cooling bias in our measurements. Finally, given the number of time series models that we needed to fit, we were unable to run diagnostic measures on the models that we did have. For example, we do not know if the roots of the ARIMA polynomial approach the edge of the unit circle for every single case. We could potentially have included such a diagnostic plot in the app, but the point of this project was to keep the statistical machinery under the hood and keep the app as approachable and non-technical as possible. It is also possible that future work could use a linear mixed effects model to attempt to get at some of the seasonality, as well as correlated nature of the data, rather than use time series techniques. This would give us a better idea of the average change in temperature over time, and lead to a more interpretable output.

### Big Data Applications

When handling 'Big Data', computational efficiency is key for faster loading times. When coding the app, we avoided nested loops, benchmarked our functions, and eliminated any necessary code to account for this. We used data.table::fread() to decrease the amount of memory used and the time it takes to read the data. This cut the time it takes to launch the app in half. 

Additionally, for visualizing such large data, we made choices to reduce the burden on the user, such as including all data points in a forecast model, but not displaying all points. That tends to get very hard to discern trend over time.

```{r, warning=FALSE}
library(microbenchmark)

benchmark <- microbenchmark(
  readCSV = utils::read.csv("GlobalLandTemperaturesByCity.csv"),
  readrCSV = readr::read_csv("GlobalLandTemperaturesByCity.csv", progress = F),
  fread = data.table::fread("GlobalLandTemperaturesByCity.csv", showProgress = F),
  times = 2
)
print(benchmark, signif = 2)
```


# Results

### GitHub Link

https://github.com/flynnmc15/environmental_shiny

### App link


# Contributions

Every group member utilized github to collaborate on the project, and worked on the RMarkdown report.

Flynn created the GitHub repository, and built the RShiny app, including the data processing, user inputs, average temperature figures, and download features.

Abby implemented the ARIMA model, and used forecast to produce the temperature prediction figures. She also made presentation slides.

Grace wrote the Rmarkdown report and benchmark file and made presentation slides.

